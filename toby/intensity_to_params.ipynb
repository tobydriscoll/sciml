{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data and create datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n"
     ]
    }
   ],
   "source": [
    "datastore = h5py.File('trials6.h5', 'r')\n",
    "data_I = torch.tensor(datastore[\"I\"])\n",
    "data_params = torch.tensor(datastore[\"P\"])\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "print(f\"Using {device} device\")\n",
    "data_I = data_I.to(device)\n",
    "data_params = data_params.to(device)\n",
    "dataset = TensorDataset(data_I, data_params)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VarTransform(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VarTransform, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # exp of the 4th input value; pass others unchanged\n",
    "        x[:, 3] = torch.exp(x[:, 3])\n",
    "        return x\n",
    "\n",
    "class net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(101, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 4),\n",
    "        )\n",
    "        self.output_transform = VarTransform()\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.layers(x)\n",
    "        y = self.output_transform(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "net(\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=101, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=256, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=256, out_features=4, bias=True)\n",
      "  )\n",
      "  (output_transform): VarTransform()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = net().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test model on an input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0405, -0.0222,  0.0407,  1.0163],\n",
       "        [-0.0399, -0.0245,  0.0407,  1.0127],\n",
       "        [-0.0447, -0.0297,  0.0445,  1.0014],\n",
       "        [-0.0417, -0.0229,  0.0411,  1.0170],\n",
       "        [-0.0405, -0.0238,  0.0410,  1.0136],\n",
       "        [-0.0404, -0.0241,  0.0413,  1.0142],\n",
       "        [-0.0423, -0.0223,  0.0426,  1.0102],\n",
       "        [-0.0441, -0.0239,  0.0411,  1.0119],\n",
       "        [-0.0445, -0.0242,  0.0422,  1.0053],\n",
       "        [-0.0423, -0.0228,  0.0389,  1.0144],\n",
       "        [-0.0421, -0.0213,  0.0379,  1.0142],\n",
       "        [-0.0418, -0.0224,  0.0407,  1.0169],\n",
       "        [-0.0422, -0.0219,  0.0410,  1.0170],\n",
       "        [-0.0439, -0.0235,  0.0404,  1.0129],\n",
       "        [-0.0498, -0.0344,  0.0534,  1.0003],\n",
       "        [-0.0419, -0.0218,  0.0409,  1.0165],\n",
       "        [-0.0425, -0.0207,  0.0409,  1.0140],\n",
       "        [-0.0403, -0.0239,  0.0415,  1.0174],\n",
       "        [-0.0592, -0.0577,  0.0608,  0.9907],\n",
       "        [-0.0401, -0.0239,  0.0416,  1.0166],\n",
       "        [-0.0479, -0.0343,  0.0487,  0.9959],\n",
       "        [-0.0433, -0.0242,  0.0419,  1.0067],\n",
       "        [-0.0430, -0.0215,  0.0422,  1.0098],\n",
       "        [-0.0434, -0.0210,  0.0392,  1.0126],\n",
       "        [-0.0400, -0.0246,  0.0411,  1.0137],\n",
       "        [-0.0430, -0.0229,  0.0426,  1.0074],\n",
       "        [-0.0400, -0.0245,  0.0410,  1.0130],\n",
       "        [-0.0416, -0.0231,  0.0410,  1.0167],\n",
       "        [-0.0557, -0.0498,  0.0625,  0.9909],\n",
       "        [-0.0495, -0.0342,  0.0504,  0.9967],\n",
       "        [-0.0419, -0.0224,  0.0405,  1.0162],\n",
       "        [-0.0395, -0.0244,  0.0414,  1.0120],\n",
       "        [-0.0488, -0.0291,  0.0466,  1.0034],\n",
       "        [-0.0463, -0.0265,  0.0449,  1.0037],\n",
       "        [-0.0447, -0.0218,  0.0401,  1.0113],\n",
       "        [-0.0443, -0.0232,  0.0410,  1.0120],\n",
       "        [-0.0402, -0.0245,  0.0411,  1.0141],\n",
       "        [-0.0512, -0.0381,  0.0564,  0.9987],\n",
       "        [-0.0424, -0.0207,  0.0407,  1.0137],\n",
       "        [-0.0434, -0.0212,  0.0381,  1.0153],\n",
       "        [-0.0427, -0.0207,  0.0407,  1.0143],\n",
       "        [-0.0408, -0.0238,  0.0412,  1.0154],\n",
       "        [-0.0409, -0.0237,  0.0411,  1.0150],\n",
       "        [-0.0411, -0.0233,  0.0415,  1.0168],\n",
       "        [-0.0555, -0.0491,  0.0624,  0.9912],\n",
       "        [-0.0469, -0.0297,  0.0471,  1.0002],\n",
       "        [-0.0404, -0.0242,  0.0412,  1.0149],\n",
       "        [-0.0471, -0.0275,  0.0454,  1.0034],\n",
       "        [-0.0560, -0.0503,  0.0603,  0.9941],\n",
       "        [-0.0515, -0.0388,  0.0568,  0.9981],\n",
       "        [-0.0422, -0.0210,  0.0411,  1.0145],\n",
       "        [-0.0397, -0.0244,  0.0411,  1.0126],\n",
       "        [-0.0425, -0.0209,  0.0378,  1.0139],\n",
       "        [-0.0450, -0.0301,  0.0447,  1.0009],\n",
       "        [-0.0399, -0.0241,  0.0414,  1.0128],\n",
       "        [-0.0403, -0.0245,  0.0411,  1.0146],\n",
       "        [-0.0427, -0.0208,  0.0414,  1.0114],\n",
       "        [-0.0427, -0.0206,  0.0408,  1.0147],\n",
       "        [-0.0640, -0.0680,  0.0610,  0.9848],\n",
       "        [-0.0400, -0.0235,  0.0412,  1.0184],\n",
       "        [-0.0538, -0.0442,  0.0585,  0.9978],\n",
       "        [-0.0456, -0.0317,  0.0465,  0.9983],\n",
       "        [-0.0399, -0.0228,  0.0416,  1.0166],\n",
       "        [-0.0398, -0.0242,  0.0413,  1.0128]], device='mps:0',\n",
       "       grad_fn=<CopySlices>)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(next(iter(train_dataloader))[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    # Set the model to training mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * batch_size + len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    # Set the model to evaluation mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
    "    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    print(f\"Test Error: Avg loss = {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 150.067703  [   64/32000]\n",
      "loss: 8.117894  [ 6464/32000]\n",
      "loss: 13.442259  [12864/32000]\n",
      "loss: 14.177588  [19264/32000]\n",
      "loss: 7.886977  [25664/32000]\n",
      "Test Error: Avg loss = 10.256440 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 5.680380  [   64/32000]\n",
      "loss: 4.025805  [ 6464/32000]\n",
      "loss: 9.031260  [12864/32000]\n",
      "loss: 5.030684  [19264/32000]\n",
      "loss: 2.118362  [25664/32000]\n",
      "Test Error: Avg loss = 4.500631 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 3.046014  [   64/32000]\n",
      "loss: 2.422184  [ 6464/32000]\n",
      "loss: 3.088905  [12864/32000]\n",
      "loss: 2.607656  [19264/32000]\n",
      "loss: 2.784999  [25664/32000]\n",
      "Test Error: Avg loss = 2.242556 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 2.650748  [   64/32000]\n",
      "loss: 0.975563  [ 6464/32000]\n",
      "loss: 1.559389  [12864/32000]\n",
      "loss: 1.554438  [19264/32000]\n",
      "loss: 1.532409  [25664/32000]\n",
      "Test Error: Avg loss = 1.627382 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 2.820386  [   64/32000]\n",
      "loss: 1.253068  [ 6464/32000]\n",
      "loss: 0.748201  [12864/32000]\n",
      "loss: 2.263379  [19264/32000]\n",
      "loss: 1.135316  [25664/32000]\n",
      "Test Error: Avg loss = 0.902343 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.925810  [   64/32000]\n",
      "loss: 0.991852  [ 6464/32000]\n",
      "loss: 0.795026  [12864/32000]\n",
      "loss: 0.863596  [19264/32000]\n",
      "loss: 0.895068  [25664/32000]\n",
      "Test Error: Avg loss = 0.669486 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.908052  [   64/32000]\n",
      "loss: 1.241593  [ 6464/32000]\n",
      "loss: 0.770787  [12864/32000]\n",
      "loss: 0.705407  [19264/32000]\n",
      "loss: 1.144152  [25664/32000]\n",
      "Test Error: Avg loss = 0.429767 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.267066  [   64/32000]\n",
      "loss: 0.448148  [ 6464/32000]\n",
      "loss: 0.721224  [12864/32000]\n",
      "loss: 0.786533  [19264/32000]\n",
      "loss: 0.438421  [25664/32000]\n",
      "Test Error: Avg loss = 0.424244 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.363832  [   64/32000]\n",
      "loss: 0.345487  [ 6464/32000]\n",
      "loss: 0.386385  [12864/32000]\n",
      "loss: 0.540953  [19264/32000]\n",
      "loss: 0.190420  [25664/32000]\n",
      "Test Error: Avg loss = 0.353424 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.373075  [   64/32000]\n",
      "loss: 0.268330  [ 6464/32000]\n",
      "loss: 0.494273  [12864/32000]\n",
      "loss: 0.348011  [19264/32000]\n",
      "loss: 0.571690  [25664/32000]\n",
      "Test Error: Avg loss = 0.399100 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.347716  [   64/32000]\n",
      "loss: 0.233121  [ 6464/32000]\n",
      "loss: 1.085050  [12864/32000]\n",
      "loss: 0.308629  [19264/32000]\n",
      "loss: 0.415079  [25664/32000]\n",
      "Test Error: Avg loss = 0.366232 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.557039  [   64/32000]\n",
      "loss: 0.162354  [ 6464/32000]\n",
      "loss: 0.388754  [12864/32000]\n",
      "loss: 0.726058  [19264/32000]\n",
      "loss: 0.441177  [25664/32000]\n",
      "Test Error: Avg loss = 0.290263 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.338065  [   64/32000]\n",
      "loss: 0.210699  [ 6464/32000]\n",
      "loss: 0.344245  [12864/32000]\n",
      "loss: 0.344924  [19264/32000]\n",
      "loss: 0.265350  [25664/32000]\n",
      "Test Error: Avg loss = 0.296282 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.259505  [   64/32000]\n",
      "loss: 0.430956  [ 6464/32000]\n",
      "loss: 0.343507  [12864/32000]\n",
      "loss: 0.249626  [19264/32000]\n",
      "loss: 0.238862  [25664/32000]\n",
      "Test Error: Avg loss = 0.328718 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.344759  [   64/32000]\n",
      "loss: 0.221974  [ 6464/32000]\n",
      "loss: 0.141325  [12864/32000]\n",
      "loss: 0.124353  [19264/32000]\n",
      "loss: 0.161659  [25664/32000]\n",
      "Test Error: Avg loss = 0.270579 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.301226  [   64/32000]\n",
      "loss: 0.311972  [ 6464/32000]\n",
      "loss: 0.334593  [12864/32000]\n",
      "loss: 0.212058  [19264/32000]\n",
      "loss: 0.347904  [25664/32000]\n",
      "Test Error: Avg loss = 0.372155 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.341709  [   64/32000]\n",
      "loss: 0.560021  [ 6464/32000]\n",
      "loss: 0.186205  [12864/32000]\n",
      "loss: 0.188811  [19264/32000]\n",
      "loss: 0.438989  [25664/32000]\n",
      "Test Error: Avg loss = 0.173172 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.169245  [   64/32000]\n",
      "loss: 0.315551  [ 6464/32000]\n",
      "loss: 0.093476  [12864/32000]\n",
      "loss: 0.181137  [19264/32000]\n",
      "loss: 0.214011  [25664/32000]\n",
      "Test Error: Avg loss = 0.461581 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.443691  [   64/32000]\n",
      "loss: 0.234054  [ 6464/32000]\n",
      "loss: 0.104259  [12864/32000]\n",
      "loss: 0.188286  [19264/32000]\n",
      "loss: 0.132585  [25664/32000]\n",
      "Test Error: Avg loss = 0.215961 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.307050  [   64/32000]\n",
      "loss: 0.343714  [ 6464/32000]\n",
      "loss: 0.180599  [12864/32000]\n",
      "loss: 0.159021  [19264/32000]\n",
      "loss: 0.199453  [25664/32000]\n",
      "Test Error: Avg loss = 0.226504 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 0.174201  [   64/32000]\n",
      "loss: 0.110609  [ 6464/32000]\n",
      "loss: 0.313090  [12864/32000]\n",
      "loss: 0.206637  [19264/32000]\n",
      "loss: 0.225811  [25664/32000]\n",
      "Test Error: Avg loss = 0.250467 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 0.183863  [   64/32000]\n",
      "loss: 0.141001  [ 6464/32000]\n",
      "loss: 0.414529  [12864/32000]\n",
      "loss: 0.159606  [19264/32000]\n",
      "loss: 0.186807  [25664/32000]\n",
      "Test Error: Avg loss = 0.134715 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 0.131897  [   64/32000]\n",
      "loss: 0.144003  [ 6464/32000]\n",
      "loss: 0.192684  [12864/32000]\n",
      "loss: 0.134048  [19264/32000]\n",
      "loss: 0.175268  [25664/32000]\n",
      "Test Error: Avg loss = 0.665531 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 0.472222  [   64/32000]\n",
      "loss: 0.119985  [ 6464/32000]\n",
      "loss: 0.237404  [12864/32000]\n",
      "loss: 0.477808  [19264/32000]\n",
      "loss: 0.227519  [25664/32000]\n",
      "Test Error: Avg loss = 0.148128 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 0.298824  [   64/32000]\n",
      "loss: 0.118187  [ 6464/32000]\n",
      "loss: 0.179718  [12864/32000]\n",
      "loss: 0.327367  [19264/32000]\n",
      "loss: 0.130291  [25664/32000]\n",
      "Test Error: Avg loss = 0.165109 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 0.175331  [   64/32000]\n",
      "loss: 0.188902  [ 6464/32000]\n",
      "loss: 0.330543  [12864/32000]\n",
      "loss: 0.105984  [19264/32000]\n",
      "loss: 0.128339  [25664/32000]\n",
      "Test Error: Avg loss = 0.147150 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 0.141522  [   64/32000]\n",
      "loss: 0.117392  [ 6464/32000]\n",
      "loss: 0.142033  [12864/32000]\n",
      "loss: 0.073835  [19264/32000]\n",
      "loss: 0.298574  [25664/32000]\n",
      "Test Error: Avg loss = 0.139663 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 0.078615  [   64/32000]\n",
      "loss: 0.103155  [ 6464/32000]\n",
      "loss: 0.207119  [12864/32000]\n",
      "loss: 0.155627  [19264/32000]\n",
      "loss: 0.403322  [25664/32000]\n",
      "Test Error: Avg loss = 0.283249 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 0.324589  [   64/32000]\n",
      "loss: 0.264940  [ 6464/32000]\n",
      "loss: 0.183039  [12864/32000]\n",
      "loss: 0.094171  [19264/32000]\n",
      "loss: 0.101505  [25664/32000]\n",
      "Test Error: Avg loss = 0.225579 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 0.200270  [   64/32000]\n",
      "loss: 0.120510  [ 6464/32000]\n",
      "loss: 0.112650  [12864/32000]\n",
      "loss: 0.205344  [19264/32000]\n",
      "loss: 0.112589  [25664/32000]\n",
      "Test Error: Avg loss = 0.127780 \n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 0.104894  [   64/32000]\n",
      "loss: 0.203115  [ 6464/32000]\n",
      "loss: 0.197024  [12864/32000]\n",
      "loss: 0.370256  [19264/32000]\n",
      "loss: 0.106421  [25664/32000]\n",
      "Test Error: Avg loss = 0.153575 \n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 0.157482  [   64/32000]\n",
      "loss: 0.120640  [ 6464/32000]\n",
      "loss: 0.152305  [12864/32000]\n",
      "loss: 0.079706  [19264/32000]\n",
      "loss: 0.240342  [25664/32000]\n",
      "Test Error: Avg loss = 0.149569 \n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 0.185357  [   64/32000]\n",
      "loss: 0.259770  [ 6464/32000]\n",
      "loss: 0.288589  [12864/32000]\n",
      "loss: 0.081815  [19264/32000]\n",
      "loss: 0.122408  [25664/32000]\n",
      "Test Error: Avg loss = 0.164948 \n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 0.216355  [   64/32000]\n",
      "loss: 0.150285  [ 6464/32000]\n",
      "loss: 0.106658  [12864/32000]\n",
      "loss: 0.162447  [19264/32000]\n",
      "loss: 0.295919  [25664/32000]\n",
      "Test Error: Avg loss = 0.183910 \n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 0.227585  [   64/32000]\n",
      "loss: 0.099838  [ 6464/32000]\n",
      "loss: 0.101369  [12864/32000]\n",
      "loss: 0.077424  [19264/32000]\n",
      "loss: 0.078368  [25664/32000]\n",
      "Test Error: Avg loss = 0.115165 \n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 0.080789  [   64/32000]\n",
      "loss: 0.131554  [ 6464/32000]\n",
      "loss: 0.145167  [12864/32000]\n",
      "loss: 0.117013  [19264/32000]\n",
      "loss: 0.243910  [25664/32000]\n",
      "Test Error: Avg loss = 0.123655 \n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 0.217405  [   64/32000]\n",
      "loss: 0.094180  [ 6464/32000]\n",
      "loss: 0.109575  [12864/32000]\n",
      "loss: 0.090304  [19264/32000]\n",
      "loss: 0.186129  [25664/32000]\n",
      "Test Error: Avg loss = 0.150684 \n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss: 0.130818  [   64/32000]\n",
      "loss: 0.110200  [ 6464/32000]\n",
      "loss: 0.155860  [12864/32000]\n",
      "loss: 0.135937  [19264/32000]\n",
      "loss: 0.063508  [25664/32000]\n",
      "Test Error: Avg loss = 0.120590 \n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 0.060778  [   64/32000]\n",
      "loss: 0.119198  [ 6464/32000]\n",
      "loss: 0.142841  [12864/32000]\n",
      "loss: 0.123501  [19264/32000]\n",
      "loss: 0.245515  [25664/32000]\n",
      "Test Error: Avg loss = 0.125773 \n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 0.086755  [   64/32000]\n",
      "loss: 0.106579  [ 6464/32000]\n",
      "loss: 0.095654  [12864/32000]\n",
      "loss: 0.140536  [19264/32000]\n",
      "loss: 0.301407  [25664/32000]\n",
      "Test Error: Avg loss = 0.084102 \n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "loss: 0.076090  [   64/32000]\n",
      "loss: 0.102117  [ 6464/32000]\n",
      "loss: 0.068232  [12864/32000]\n",
      "loss: 0.172612  [19264/32000]\n",
      "loss: 0.141554  [25664/32000]\n",
      "Test Error: Avg loss = 0.153088 \n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "loss: 0.130540  [   64/32000]\n",
      "loss: 0.232266  [ 6464/32000]\n",
      "loss: 0.141183  [12864/32000]\n",
      "loss: 0.062835  [19264/32000]\n",
      "loss: 0.093789  [25664/32000]\n",
      "Test Error: Avg loss = 0.103543 \n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "loss: 0.083712  [   64/32000]\n",
      "loss: 0.165580  [ 6464/32000]\n",
      "loss: 0.312789  [12864/32000]\n",
      "loss: 0.079588  [19264/32000]\n",
      "loss: 0.050835  [25664/32000]\n",
      "Test Error: Avg loss = 0.108831 \n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "loss: 0.104927  [   64/32000]\n",
      "loss: 0.187567  [ 6464/32000]\n",
      "loss: 0.059427  [12864/32000]\n",
      "loss: 0.097593  [19264/32000]\n",
      "loss: 0.285576  [25664/32000]\n",
      "Test Error: Avg loss = 0.123461 \n",
      "\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "loss: 0.092649  [   64/32000]\n",
      "loss: 0.101948  [ 6464/32000]\n",
      "loss: 0.078743  [12864/32000]\n",
      "loss: 0.118690  [19264/32000]\n",
      "loss: 0.093939  [25664/32000]\n",
      "Test Error: Avg loss = 0.160335 \n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "loss: 0.145079  [   64/32000]\n",
      "loss: 0.077152  [ 6464/32000]\n",
      "loss: 0.176948  [12864/32000]\n",
      "loss: 0.158940  [19264/32000]\n",
      "loss: 0.263254  [25664/32000]\n",
      "Test Error: Avg loss = 0.114015 \n",
      "\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "loss: 0.080797  [   64/32000]\n",
      "loss: 0.136849  [ 6464/32000]\n",
      "loss: 0.097154  [12864/32000]\n",
      "loss: 0.131092  [19264/32000]\n",
      "loss: 0.110381  [25664/32000]\n",
      "Test Error: Avg loss = 0.079649 \n",
      "\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "loss: 0.064283  [   64/32000]\n",
      "loss: 0.274721  [ 6464/32000]\n",
      "loss: 0.102454  [12864/32000]\n",
      "loss: 0.135381  [19264/32000]\n",
      "loss: 0.068835  [25664/32000]\n",
      "Test Error: Avg loss = 0.153906 \n",
      "\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "loss: 0.112066  [   64/32000]\n",
      "loss: 0.146361  [ 6464/32000]\n",
      "loss: 0.257036  [12864/32000]\n",
      "loss: 0.172532  [19264/32000]\n",
      "loss: 0.085256  [25664/32000]\n",
      "Test Error: Avg loss = 0.111103 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "loss: 0.082289  [   64/32000]\n",
      "loss: 0.406887  [ 6464/32000]\n",
      "loss: 0.091035  [12864/32000]\n",
      "loss: 0.079094  [19264/32000]\n",
      "loss: 0.198365  [25664/32000]\n",
      "Test Error: Avg loss = 0.117032 \n",
      "\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "loss: 0.109813  [   64/32000]\n",
      "loss: 0.093211  [ 6464/32000]\n",
      "loss: 0.112691  [12864/32000]\n",
      "loss: 0.112942  [19264/32000]\n",
      "loss: 0.095988  [25664/32000]\n",
      "Test Error: Avg loss = 0.103875 \n",
      "\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "loss: 0.155053  [   64/32000]\n",
      "loss: 0.117055  [ 6464/32000]\n",
      "loss: 0.159266  [12864/32000]\n",
      "loss: 0.060533  [19264/32000]\n",
      "loss: 0.127820  [25664/32000]\n",
      "Test Error: Avg loss = 0.122168 \n",
      "\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "loss: 0.105553  [   64/32000]\n",
      "loss: 0.184547  [ 6464/32000]\n",
      "loss: 0.129227  [12864/32000]\n",
      "loss: 0.138786  [19264/32000]\n",
      "loss: 0.101415  [25664/32000]\n",
      "Test Error: Avg loss = 0.116360 \n",
      "\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "loss: 0.134180  [   64/32000]\n",
      "loss: 0.158884  [ 6464/32000]\n",
      "loss: 0.062493  [12864/32000]\n",
      "loss: 0.123368  [19264/32000]\n",
      "loss: 0.081231  [25664/32000]\n",
      "Test Error: Avg loss = 0.151083 \n",
      "\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "loss: 0.154029  [   64/32000]\n",
      "loss: 0.065033  [ 6464/32000]\n",
      "loss: 0.099960  [12864/32000]\n",
      "loss: 0.091931  [19264/32000]\n",
      "loss: 0.065664  [25664/32000]\n",
      "Test Error: Avg loss = 0.148233 \n",
      "\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "loss: 0.101303  [   64/32000]\n",
      "loss: 0.065255  [ 6464/32000]\n",
      "loss: 0.092314  [12864/32000]\n",
      "loss: 0.067258  [19264/32000]\n",
      "loss: 0.118198  [25664/32000]\n",
      "Test Error: Avg loss = 0.094312 \n",
      "\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "loss: 0.077726  [   64/32000]\n",
      "loss: 0.068499  [ 6464/32000]\n",
      "loss: 0.072209  [12864/32000]\n",
      "loss: 0.124993  [19264/32000]\n",
      "loss: 0.069123  [25664/32000]\n",
      "Test Error: Avg loss = 0.124304 \n",
      "\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "loss: 0.120510  [   64/32000]\n",
      "loss: 0.100063  [ 6464/32000]\n",
      "loss: 0.028178  [12864/32000]\n",
      "loss: 0.175844  [19264/32000]\n",
      "loss: 0.097657  [25664/32000]\n",
      "Test Error: Avg loss = 0.084599 \n",
      "\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "loss: 0.084538  [   64/32000]\n",
      "loss: 0.063879  [ 6464/32000]\n",
      "loss: 0.145593  [12864/32000]\n",
      "loss: 0.113524  [19264/32000]\n",
      "loss: 0.174259  [25664/32000]\n",
      "Test Error: Avg loss = 0.116486 \n",
      "\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "loss: 0.142460  [   64/32000]\n",
      "loss: 0.085063  [ 6464/32000]\n",
      "loss: 0.066924  [12864/32000]\n",
      "loss: 0.113649  [19264/32000]\n",
      "loss: 0.059413  [25664/32000]\n",
      "Test Error: Avg loss = 0.102278 \n",
      "\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "loss: 0.093941  [   64/32000]\n",
      "loss: 0.069426  [ 6464/32000]\n",
      "loss: 0.101400  [12864/32000]\n",
      "loss: 0.172184  [19264/32000]\n",
      "loss: 0.120036  [25664/32000]\n",
      "Test Error: Avg loss = 0.076718 \n",
      "\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "loss: 0.086304  [   64/32000]\n",
      "loss: 0.108076  [ 6464/32000]\n",
      "loss: 0.094993  [12864/32000]\n",
      "loss: 0.102156  [19264/32000]\n",
      "loss: 0.113139  [25664/32000]\n",
      "Test Error: Avg loss = 0.141828 \n",
      "\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "loss: 0.088922  [   64/32000]\n",
      "loss: 0.126073  [ 6464/32000]\n",
      "loss: 0.061958  [12864/32000]\n",
      "loss: 0.093654  [19264/32000]\n",
      "loss: 0.080396  [25664/32000]\n",
      "Test Error: Avg loss = 0.079086 \n",
      "\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "loss: 0.047663  [   64/32000]\n",
      "loss: 0.079290  [ 6464/32000]\n",
      "loss: 0.127835  [12864/32000]\n",
      "loss: 0.060759  [19264/32000]\n",
      "loss: 0.089024  [25664/32000]\n",
      "Test Error: Avg loss = 0.107070 \n",
      "\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "loss: 0.090358  [   64/32000]\n",
      "loss: 0.126745  [ 6464/32000]\n",
      "loss: 0.102941  [12864/32000]\n",
      "loss: 0.086176  [19264/32000]\n",
      "loss: 0.170185  [25664/32000]\n",
      "Test Error: Avg loss = 0.118858 \n",
      "\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "loss: 0.123737  [   64/32000]\n",
      "loss: 0.158601  [ 6464/32000]\n",
      "loss: 0.117603  [12864/32000]\n",
      "loss: 0.132849  [19264/32000]\n",
      "loss: 0.096166  [25664/32000]\n",
      "Test Error: Avg loss = 0.113581 \n",
      "\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "loss: 0.156168  [   64/32000]\n",
      "loss: 0.176591  [ 6464/32000]\n",
      "loss: 0.147995  [12864/32000]\n",
      "loss: 0.055502  [19264/32000]\n",
      "loss: 0.095631  [25664/32000]\n",
      "Test Error: Avg loss = 0.098071 \n",
      "\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "loss: 0.073708  [   64/32000]\n",
      "loss: 0.067848  [ 6464/32000]\n",
      "loss: 0.134193  [12864/32000]\n",
      "loss: 0.055590  [19264/32000]\n",
      "loss: 0.063884  [25664/32000]\n",
      "Test Error: Avg loss = 0.114773 \n",
      "\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "loss: 0.071616  [   64/32000]\n",
      "loss: 0.146444  [ 6464/32000]\n",
      "loss: 0.051733  [12864/32000]\n",
      "loss: 0.069267  [19264/32000]\n",
      "loss: 0.129554  [25664/32000]\n",
      "Test Error: Avg loss = 0.082370 \n",
      "\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "loss: 0.089860  [   64/32000]\n",
      "loss: 0.072917  [ 6464/32000]\n",
      "loss: 0.079365  [12864/32000]\n",
      "loss: 0.056546  [19264/32000]\n",
      "loss: 0.093621  [25664/32000]\n",
      "Test Error: Avg loss = 0.125764 \n",
      "\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "loss: 0.065769  [   64/32000]\n",
      "loss: 0.054872  [ 6464/32000]\n",
      "loss: 0.085630  [12864/32000]\n",
      "loss: 0.148053  [19264/32000]\n",
      "loss: 0.069486  [25664/32000]\n",
      "Test Error: Avg loss = 0.104063 \n",
      "\n",
      "Epoch 72\n",
      "-------------------------------\n",
      "loss: 0.075975  [   64/32000]\n",
      "loss: 0.069447  [ 6464/32000]\n",
      "loss: 0.088666  [12864/32000]\n",
      "loss: 0.116960  [19264/32000]\n",
      "loss: 0.132430  [25664/32000]\n",
      "Test Error: Avg loss = 0.089668 \n",
      "\n",
      "Epoch 73\n",
      "-------------------------------\n",
      "loss: 0.103198  [   64/32000]\n",
      "loss: 0.092626  [ 6464/32000]\n",
      "loss: 0.048354  [12864/32000]\n",
      "loss: 0.097594  [19264/32000]\n",
      "loss: 0.045748  [25664/32000]\n",
      "Test Error: Avg loss = 0.076308 \n",
      "\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "loss: 0.069784  [   64/32000]\n",
      "loss: 0.106660  [ 6464/32000]\n",
      "loss: 0.089110  [12864/32000]\n",
      "loss: 0.078934  [19264/32000]\n",
      "loss: 0.076139  [25664/32000]\n",
      "Test Error: Avg loss = 0.088849 \n",
      "\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "loss: 0.076114  [   64/32000]\n",
      "loss: 0.120711  [ 6464/32000]\n",
      "loss: 0.082765  [12864/32000]\n",
      "loss: 0.098213  [19264/32000]\n",
      "loss: 0.085689  [25664/32000]\n",
      "Test Error: Avg loss = 0.077212 \n",
      "\n",
      "Epoch 76\n",
      "-------------------------------\n",
      "loss: 0.124752  [   64/32000]\n",
      "loss: 0.063735  [ 6464/32000]\n",
      "loss: 0.152726  [12864/32000]\n",
      "loss: 0.058284  [19264/32000]\n",
      "loss: 0.043969  [25664/32000]\n",
      "Test Error: Avg loss = 0.108726 \n",
      "\n",
      "Epoch 77\n",
      "-------------------------------\n",
      "loss: 0.103276  [   64/32000]\n",
      "loss: 0.150891  [ 6464/32000]\n",
      "loss: 0.085729  [12864/32000]\n",
      "loss: 0.039746  [19264/32000]\n",
      "loss: 0.150089  [25664/32000]\n",
      "Test Error: Avg loss = 0.108236 \n",
      "\n",
      "Epoch 78\n",
      "-------------------------------\n",
      "loss: 0.137838  [   64/32000]\n",
      "loss: 0.085957  [ 6464/32000]\n",
      "loss: 0.159788  [12864/32000]\n",
      "loss: 0.061821  [19264/32000]\n",
      "loss: 0.118805  [25664/32000]\n",
      "Test Error: Avg loss = 0.116429 \n",
      "\n",
      "Epoch 79\n",
      "-------------------------------\n",
      "loss: 0.084744  [   64/32000]\n",
      "loss: 0.059620  [ 6464/32000]\n",
      "loss: 0.112091  [12864/32000]\n",
      "loss: 0.172280  [19264/32000]\n",
      "loss: 0.034505  [25664/32000]\n",
      "Test Error: Avg loss = 0.084028 \n",
      "\n",
      "Epoch 80\n",
      "-------------------------------\n",
      "loss: 0.266122  [   64/32000]\n",
      "loss: 0.030121  [ 6464/32000]\n",
      "loss: 0.109706  [12864/32000]\n",
      "loss: 0.105998  [19264/32000]\n",
      "loss: 0.050444  [25664/32000]\n",
      "Test Error: Avg loss = 0.098900 \n",
      "\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "loss: 0.082302  [   64/32000]\n",
      "loss: 0.105119  [ 6464/32000]\n",
      "loss: 0.074805  [12864/32000]\n",
      "loss: 0.096857  [19264/32000]\n",
      "loss: 0.030048  [25664/32000]\n",
      "Test Error: Avg loss = 0.135128 \n",
      "\n",
      "Epoch 82\n",
      "-------------------------------\n",
      "loss: 0.155986  [   64/32000]\n",
      "loss: 0.109813  [ 6464/32000]\n",
      "loss: 0.087230  [12864/32000]\n",
      "loss: 0.079560  [19264/32000]\n",
      "loss: 0.072495  [25664/32000]\n",
      "Test Error: Avg loss = 0.158719 \n",
      "\n",
      "Epoch 83\n",
      "-------------------------------\n",
      "loss: 0.156268  [   64/32000]\n",
      "loss: 0.130628  [ 6464/32000]\n",
      "loss: 0.217623  [12864/32000]\n",
      "loss: 0.080275  [19264/32000]\n",
      "loss: 0.052630  [25664/32000]\n",
      "Test Error: Avg loss = 0.091181 \n",
      "\n",
      "Epoch 84\n",
      "-------------------------------\n",
      "loss: 0.121026  [   64/32000]\n",
      "loss: 0.122932  [ 6464/32000]\n",
      "loss: 0.094577  [12864/32000]\n",
      "loss: 0.075630  [19264/32000]\n",
      "loss: 0.074336  [25664/32000]\n",
      "Test Error: Avg loss = 0.062773 \n",
      "\n",
      "Epoch 85\n",
      "-------------------------------\n",
      "loss: 0.049005  [   64/32000]\n",
      "loss: 0.071342  [ 6464/32000]\n",
      "loss: 0.074431  [12864/32000]\n",
      "loss: 0.066917  [19264/32000]\n",
      "loss: 0.099606  [25664/32000]\n",
      "Test Error: Avg loss = 0.099637 \n",
      "\n",
      "Epoch 86\n",
      "-------------------------------\n",
      "loss: 0.106832  [   64/32000]\n",
      "loss: 0.062085  [ 6464/32000]\n",
      "loss: 0.194078  [12864/32000]\n",
      "loss: 0.127373  [19264/32000]\n",
      "loss: 0.081841  [25664/32000]\n",
      "Test Error: Avg loss = 0.099402 \n",
      "\n",
      "Epoch 87\n",
      "-------------------------------\n",
      "loss: 0.080341  [   64/32000]\n",
      "loss: 0.041526  [ 6464/32000]\n",
      "loss: 0.035686  [12864/32000]\n",
      "loss: 0.138866  [19264/32000]\n",
      "loss: 0.057511  [25664/32000]\n",
      "Test Error: Avg loss = 0.130170 \n",
      "\n",
      "Epoch 88\n",
      "-------------------------------\n",
      "loss: 0.130516  [   64/32000]\n",
      "loss: 0.035644  [ 6464/32000]\n",
      "loss: 0.094404  [12864/32000]\n",
      "loss: 0.143845  [19264/32000]\n",
      "loss: 0.051534  [25664/32000]\n",
      "Test Error: Avg loss = 0.062099 \n",
      "\n",
      "Epoch 89\n",
      "-------------------------------\n",
      "loss: 0.052636  [   64/32000]\n",
      "loss: 0.060618  [ 6464/32000]\n",
      "loss: 0.233210  [12864/32000]\n",
      "loss: 0.064740  [19264/32000]\n",
      "loss: 0.120483  [25664/32000]\n",
      "Test Error: Avg loss = 0.074298 \n",
      "\n",
      "Epoch 90\n",
      "-------------------------------\n",
      "loss: 0.046474  [   64/32000]\n",
      "loss: 0.129399  [ 6464/32000]\n",
      "loss: 0.087170  [12864/32000]\n",
      "loss: 0.064681  [19264/32000]\n",
      "loss: 0.168319  [25664/32000]\n",
      "Test Error: Avg loss = 0.159389 \n",
      "\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "loss: 0.158505  [   64/32000]\n",
      "loss: 0.053720  [ 6464/32000]\n",
      "loss: 0.064302  [12864/32000]\n",
      "loss: 0.100791  [19264/32000]\n",
      "loss: 0.052091  [25664/32000]\n",
      "Test Error: Avg loss = 0.059958 \n",
      "\n",
      "Epoch 92\n",
      "-------------------------------\n",
      "loss: 0.026905  [   64/32000]\n",
      "loss: 0.046311  [ 6464/32000]\n",
      "loss: 0.078771  [12864/32000]\n",
      "loss: 0.131796  [19264/32000]\n",
      "loss: 0.055168  [25664/32000]\n",
      "Test Error: Avg loss = 0.068989 \n",
      "\n",
      "Epoch 93\n",
      "-------------------------------\n",
      "loss: 0.086228  [   64/32000]\n",
      "loss: 0.099084  [ 6464/32000]\n",
      "loss: 0.078939  [12864/32000]\n",
      "loss: 0.075872  [19264/32000]\n",
      "loss: 0.042972  [25664/32000]\n",
      "Test Error: Avg loss = 0.113383 \n",
      "\n",
      "Epoch 94\n",
      "-------------------------------\n",
      "loss: 0.099676  [   64/32000]\n",
      "loss: 0.065294  [ 6464/32000]\n",
      "loss: 0.096825  [12864/32000]\n",
      "loss: 0.060204  [19264/32000]\n",
      "loss: 0.095130  [25664/32000]\n",
      "Test Error: Avg loss = 0.118513 \n",
      "\n",
      "Epoch 95\n",
      "-------------------------------\n",
      "loss: 0.179654  [   64/32000]\n",
      "loss: 0.105182  [ 6464/32000]\n",
      "loss: 0.076034  [12864/32000]\n",
      "loss: 0.051151  [19264/32000]\n",
      "loss: 0.132484  [25664/32000]\n",
      "Test Error: Avg loss = 0.088238 \n",
      "\n",
      "Epoch 96\n",
      "-------------------------------\n",
      "loss: 0.062693  [   64/32000]\n",
      "loss: 0.126599  [ 6464/32000]\n",
      "loss: 0.042720  [12864/32000]\n",
      "loss: 0.086394  [19264/32000]\n",
      "loss: 0.076407  [25664/32000]\n",
      "Test Error: Avg loss = 0.060902 \n",
      "\n",
      "Epoch 97\n",
      "-------------------------------\n",
      "loss: 0.046730  [   64/32000]\n",
      "loss: 0.058371  [ 6464/32000]\n",
      "loss: 0.099228  [12864/32000]\n",
      "loss: 0.054780  [19264/32000]\n",
      "loss: 0.088947  [25664/32000]\n",
      "Test Error: Avg loss = 0.073991 \n",
      "\n",
      "Epoch 98\n",
      "-------------------------------\n",
      "loss: 0.059924  [   64/32000]\n",
      "loss: 0.124777  [ 6464/32000]\n",
      "loss: 0.101633  [12864/32000]\n",
      "loss: 0.088407  [19264/32000]\n",
      "loss: 0.080206  [25664/32000]\n",
      "Test Error: Avg loss = 0.068640 \n",
      "\n",
      "Epoch 99\n",
      "-------------------------------\n",
      "loss: 0.048140  [   64/32000]\n",
      "loss: 0.169306  [ 6464/32000]\n",
      "loss: 0.072706  [12864/32000]\n",
      "loss: 0.061384  [19264/32000]\n",
      "loss: 0.045311  [25664/32000]\n",
      "Test Error: Avg loss = 0.052673 \n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "loss: 0.057400  [   64/32000]\n",
      "loss: 0.074484  [ 6464/32000]\n",
      "loss: 0.040339  [12864/32000]\n",
      "loss: 0.096593  [19264/32000]\n",
      "loss: 0.053941  [25664/32000]\n",
      "Test Error: Avg loss = 0.066122 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-3\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "epochs = 100\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspecting results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.1349e+01, -3.4761e-03,  1.7915e+00,  3.8139e-05],\n",
       "        [ 1.9087e+01,  2.0554e-01,  2.0395e+00,  6.6299e-04],\n",
       "        [ 2.6662e+01,  5.0118e-01,  2.2408e+00,  6.8067e-05],\n",
       "        [ 3.4879e+01,  7.9988e-01,  2.5299e+00,  2.0273e-05],\n",
       "        [ 4.8898e+00,  1.0562e+00,  2.4711e+00,  2.0854e-04],\n",
       "        [ 1.2960e+01,  1.2587e+00,  2.6819e+00,  9.7381e-04]], device='mps:0',\n",
       "       grad_fn=<CopySlices>)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(data_I[10:16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.1236e+01, 2.4793e-02, 1.9412e+00, 4.4138e-04],\n",
       "        [1.9226e+01, 2.9752e-01, 2.1176e+00, 4.7241e-04],\n",
       "        [2.7216e+01, 5.7025e-01, 2.2941e+00, 5.0345e-04],\n",
       "        [3.5206e+01, 8.4298e-01, 2.4706e+00, 5.3448e-04],\n",
       "        [4.8440e+00, 1.1157e+00, 2.6471e+00, 5.6552e-04],\n",
       "        [1.2834e+01, 1.3884e+00, 2.8235e+00, 5.9655e-04]], device='mps:0')"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_params[10:16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    pred = model(data_I).cpu().numpy()\n",
    "r = range(1000, 1100)\n",
    "plt.plot(np.vstack((r,r)), np.vstack((data_params[r, 0].cpu().numpy(), pred[r, 0])));\n",
    "plt.xlabel(\"Sample number\")\n",
    "plt.ylabel(\"actual/predicted value 0\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
